{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fdk5PXP3fnJz",
   "metadata": {
    "id": "fdk5PXP3fnJz"
   },
   "outputs": [],
   "source": [
    "# !pip install onnx onnxruntime onnxscript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b549e7e-7d22-4c98-ac22-586da5f22723",
   "metadata": {
    "id": "7b549e7e-7d22-4c98-ac22-586da5f22723"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bhuvanesh/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# --- Imports ---\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "# Optional ONNX/TensorRT imports\n",
    "# import onnx\n",
    "# import onnxscript\n",
    "# import onnxruntime as ort\n",
    "\n",
    "# --- Add project root to path (adjust as needed) ---\n",
    "PROJECT_ROOT = \"LLM/ernie-tensorrt-inference\"\n",
    "if PROJECT_ROOT not in sys.path:\n",
    "    sys.path.append(PROJECT_ROOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2DYJqsW8y_qK",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 31,
     "status": "ok",
     "timestamp": 1755786783969,
     "user": {
      "displayName": "Athiyen Arivalagan",
      "userId": "03296907817711712766"
     },
     "user_tz": -480
    },
    "id": "2DYJqsW8y_qK",
    "outputId": "628be838-87f1-4124-f9f9-281ca6902f63"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "# --- Device setup ---\n",
    "def get_device():\n",
    "    return torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "device = get_device()\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "XfQTe8PJ7kgF",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21,
     "status": "ok",
     "timestamp": 1755786783995,
     "user": {
      "displayName": "Athiyen Arivalagan",
      "userId": "03296907817711712766"
     },
     "user_tz": -480
    },
    "id": "XfQTe8PJ7kgF",
    "outputId": "c16dd362-e676-49d7-e8fd-4456e5277beb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized Model Precision: float16\n",
      "Batch size: 16\n"
     ]
    }
   ],
   "source": [
    "# --- Precision selection for optimized model ---\n",
    "PRECISION_OPTIONS = [\"float32\", \"float16\", \"bfloat16\"]\n",
    "precision = \"float16\" # change as needed\n",
    "assert precision in PRECISION_OPTIONS, f\"Precision must be one of {PRECISION_OPTIONS}\"\n",
    "print(f\"Optimized Model Precision: {precision}\")\n",
    "\n",
    "# --- Batch size selection ---\n",
    "BATCH_SIZE_OPTIONS = [16, 32, 64, 128, 512]\n",
    "batch_size = 16 # change as needed\n",
    "assert batch_size in BATCH_SIZE_OPTIONS, f\"Batch size must be one of {BATCH_SIZE_OPTIONS}\"\n",
    "print(f\"Batch size: {batch_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "XXxroDlmUKtm",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3930,
     "status": "ok",
     "timestamp": 1755786787926,
     "user": {
      "displayName": "Athiyen Arivalagan",
      "userId": "03296907817711712766"
     },
     "user_tz": -480
    },
    "id": "XXxroDlmUKtm",
    "outputId": "d5ad3d63-39db-4f4e-a1ed-3523d616c484"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation dataset size: 10000\n"
     ]
    }
   ],
   "source": [
    "# --- Load validation dataset ---\n",
    "def load_validation_dataset(dataset_name=\"C-MTEB/TNews-classification\", split=\"validation\"):\n",
    "    ds = load_dataset(dataset_name)\n",
    "    return ds[split]\n",
    "\n",
    "val_data = load_validation_dataset()\n",
    "print(f\"Validation dataset size: {len(val_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "rFk6BHqTl42a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6005,
     "status": "ok",
     "timestamp": 1755786793934,
     "user": {
      "displayName": "Athiyen Arivalagan",
      "userId": "03296907817711712766"
     },
     "user_tz": -480
    },
    "id": "rFk6BHqTl42a",
    "outputId": "7f8d1bac-26bc-49d8-a226-50cc734acf8a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ErnieForSequenceClassification were not initialized from the model checkpoint at nghuyong/ernie-3.0-base-zh and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of ErnieForSequenceClassification were not initialized from the model checkpoint at nghuyong/ernie-3.0-base-zh and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of labels: 15\n"
     ]
    }
   ],
   "source": [
    "# --- Model setup ---\n",
    "def load_models(model_name=\"nghuyong/ernie-3.0-base-zh\", num_labels=15):\n",
    "    # Original model (baseline)\n",
    "    model_orig = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n",
    "\n",
    "    # Optimized model (for future TensorRT / fused layers)\n",
    "    model_opt = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n",
    "\n",
    "    return model_orig, model_opt\n",
    "\n",
    "# --- Load models ---\n",
    "model_orig, model_opt = load_models(num_labels=15)\n",
    "\n",
    "# --- Print configuration info ---\n",
    "config = model_orig.config\n",
    "print(f\"Number of labels: {config.num_labels}\") # outputs: 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ikNblAU4YD8F",
   "metadata": {
    "id": "ikNblAU4YD8F"
   },
   "outputs": [],
   "source": [
    "from tokenizer import tokenize_function\n",
    "\n",
    "# --- Tokenize + prepare DataLoader ---\n",
    "def prepare_dataloader(dataset, batch_size=batch_size, max_length=64, shuffle=False):\n",
    "    tokenized = dataset.map(lambda x: tokenize_function(x, max_length=max_length), batched=True)\n",
    "    tokenized.set_format(\n",
    "        type=\"torch\",\n",
    "        columns=[\"input_ids\", \"attention_mask\", \"token_type_ids\", \"label\"]\n",
    "    )\n",
    "    return DataLoader(tokenized, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "# --- Create DataLoader ---\n",
    "val_loader = prepare_dataloader(val_data, batch_size=batch_size, max_length=64, shuffle=False)\n",
    "\n",
    "# Note: smaller max_length (e.g., 64 or 128 vs default 512) reduces memory and boosts inference speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9efd48d3-0ab6-462a-add7-3cfbba543946",
   "metadata": {
    "id": "9efd48d3-0ab6-462a-add7-3cfbba543946"
   },
   "outputs": [],
   "source": [
    "from models import ErnieEmbeddings, ErnieSelfAttention, ErnieSelfOutput, ErnieIntermediate, ErnieOutput, ErniePooler\n",
    "\n",
    "# --- Replace ERNIE model components for optimization ---\n",
    "def replace_ernie_layers(model, config):\n",
    "    # Replace embeddings\n",
    "    model.ernie.embeddings = ErnieEmbeddings(config)\n",
    "\n",
    "    # Replace each encoder layer's components\n",
    "    for layer in model.ernie.encoder.layer:\n",
    "        layer.attention.self = ErnieSelfAttention(config)\n",
    "        layer.attention.output = ErnieSelfOutput(config)\n",
    "        layer.intermediate = ErnieIntermediate(config)\n",
    "        layer.output = ErnieOutput(config)\n",
    "        layer.pooler = ErniePooler(config)\n",
    "\n",
    "    model.ernie.pooler = ErniePooler(config)\n",
    "\n",
    "    return model\n",
    "\n",
    "model_opt = replace_ernie_layers(model_opt, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "WuU5h0KW1qYM",
   "metadata": {
    "id": "WuU5h0KW1qYM"
   },
   "outputs": [],
   "source": [
    "# --- Helper function to prepare model ---\n",
    "def prepare_model(model, device):\n",
    "    model.eval()\n",
    "    return model.to(device)\n",
    "\n",
    "# --- Apply to original and optimized models ---\n",
    "model_orig = prepare_model(model_orig, device)\n",
    "model_opt = prepare_model(model_opt, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0af0a164-8332-441b-b997-7f5ab3ddccaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ErnieForSequenceClassification(\n",
       "  (ernie): ErnieModel(\n",
       "    (embeddings): ErnieEmbeddings(\n",
       "      (word_embeddings): Embedding(40000, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(2048, 768)\n",
       "      (token_type_embeddings): Embedding(4, 768)\n",
       "      (task_type_embeddings): Embedding(3, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): ErnieEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x ErnieLayer(\n",
       "          (attention): ErnieAttention(\n",
       "            (self): ErnieSelfAttention(\n",
       "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): ErnieSelfOutput(\n",
       "              (fused_dense_ln): FusedDenseLayerNorm(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ErnieIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ErnieOutput(\n",
       "            (fused_dense_ln): FusedDenseLayerNorm(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (pooler): ErniePooler(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (activation): Tanh()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): ErniePooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=15, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "mQXfsdWrOsaN",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22,
     "status": "ok",
     "timestamp": 1755786796489,
     "user": {
      "displayName": "Athiyen Arivalagan",
      "userId": "03296907817711712766"
     },
     "user_tz": -480
    },
    "id": "mQXfsdWrOsaN",
    "outputId": "669fa309-390e-44a8-9be0-1eb3b627c75d"
   },
   "outputs": [],
   "source": [
    "# --- Set model precision ---\n",
    "def set_model_precision(model, precision=\"float32\"):\n",
    "    if precision == \"float16\":\n",
    "        model = model.half()\n",
    "    elif precision == \"bfloat16\":\n",
    "        model = model.to(torch.bfloat16)\n",
    "    # float32 requires no action\n",
    "    return model\n",
    "\n",
    "model_opt = set_model_precision(model_opt, precision)\n",
    "\n",
    "# --- Print model info ---\n",
    "def print_model_info(model_orig, model_opt, precision):\n",
    "    print(f\"Original Model Device: {model_orig.device}\")\n",
    "    print(f\"Optimized Model Device: {model_opt.device}\")\n",
    "    print(f\"Optimized Model Precision: {model_opt.dtype} ({precision})\")\n",
    "\n",
    "# print_model_info(model_orig, model_opt, precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e70FZHkoe7gv",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22619,
     "status": "ok",
     "timestamp": 1755786819110,
     "user": {
      "displayName": "Athiyen Arivalagan",
      "userId": "03296907817711712766"
     },
     "user_tz": -480
    },
    "id": "e70FZHkoe7gv",
    "outputId": "94f63709-3b1e-40ce-9407-b98a75ab1912"
   },
   "outputs": [],
   "source": [
    "# --- Loss function ---\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# --- Metric computation ---\n",
    "def evaluate_model(model, data_loader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            # Move inputs and labels to device\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            token_type_ids = batch[\"token_type_ids\"].to(device)\n",
    "            labels = batch[\"label\"].to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                token_type_ids=token_type_ids\n",
    "            )\n",
    "\n",
    "            # Use outputs[0] for models without logits attribute\n",
    "            logits = outputs[0] # shape: [batch_size, num_classes]\n",
    "\n",
    "            # Loss\n",
    "            loss = criterion(logits, labels)\n",
    "            total_loss += loss.item() * input_ids.size(0)\n",
    "\n",
    "            # Accuracy\n",
    "            preds = logits.argmax(dim=-1)\n",
    "            total_correct += (preds == labels).sum().item()\n",
    "            total_samples += input_ids.size(0)\n",
    "\n",
    "    avg_loss = total_loss / total_samples\n",
    "    accuracy = total_correct / total_samples\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "# --- Latency benchmarking ---\n",
    "def measure_latency(model, inputs, device, n_warmup=10, n_iter=100):\n",
    "    latencies = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Warm-up\n",
    "        for _ in range(n_warmup):\n",
    "            _ = model(**inputs)\n",
    "            torch.cuda.synchronize()\n",
    "\n",
    "        # Measure\n",
    "        for _ in range(n_iter):\n",
    "            torch.cuda.synchronize()\n",
    "            start = time.time()\n",
    "            _ = model(**inputs)\n",
    "            torch.cuda.synchronize()\n",
    "            end = time.time()\n",
    "            latencies.append((end - start) * 1000) # ms\n",
    "\n",
    "    mean_latency = np.mean(latencies)\n",
    "    return mean_latency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9b5621d8-8eb2-4306-a2dd-a1ca18cfbaae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Original] Precision: torch.float32, Loss: 2.7442, Accuracy: 0.0933, Mean latency: 294.83 ms\n",
      "[Optimized] Precision: torch.float16, Loss: 2.7175, Accuracy: 0.0670, Mean latency: 1684.15 ms\n"
     ]
    }
   ],
   "source": [
    "# --- Prepare a single batch for latency measurement ---\n",
    "sample_batch = next(iter(val_loader))\n",
    "# inputs = {k: v.to(device) for k, v in sample_batch.items() if k in [\"input_ids\", \"attention_mask\", \"token_type_ids\"]}\n",
    "\n",
    "# Convert only float32 tensors to FP16, keep int64 (like input_ids) untouched\n",
    "inputs = {\n",
    "    k: (v.to(device).half() if v.dtype == torch.float32 else v.to(device))\n",
    "    for k, v in sample_batch.items()\n",
    "    if k in [\"input_ids\", \"attention_mask\", \"token_type_ids\"]}\n",
    "\n",
    "# --- Evaluate Original Model ---\n",
    "avg_loss_orig, acc_orig = evaluate_model(model_orig, val_loader, device)\n",
    "latency_orig = measure_latency(model_orig, inputs, device)\n",
    "\n",
    "print(f\"[Original] Precision: {model_orig.dtype}, Loss: {avg_loss_orig:.4f}, Accuracy: {acc_orig:.4f}, Mean latency: {latency_orig:.2f} ms\")\n",
    "\n",
    "# --- Evaluate Optimized Model ---\n",
    "avg_loss_opt, acc_opt = evaluate_model(model_opt, val_loader, device)\n",
    "latency_opt = measure_latency(model_opt, inputs, device)\n",
    "\n",
    "print(f\"[Optimized] Precision: {model_opt.dtype}, Loss: {avg_loss_opt:.4f}, Accuracy: {acc_opt:.4f}, Mean latency: {latency_opt:.2f} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "Fn8JY2apJ_rr",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3941,
     "status": "ok",
     "timestamp": 1755786823071,
     "user": {
      "displayName": "Athiyen Arivalagan",
      "userId": "03296907817711712766"
     },
     "user_tz": -480
    },
    "id": "Fn8JY2apJ_rr",
    "outputId": "f53089eb-a00e-41a1-a09e-5b719656c426"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bhuvanesh/Library/Python/3.9/lib/python/site-packages/torch/autograd/profiler.py:263: UserWarning: CUDA is not available, disabling CUDA profiling\n",
      "  warn(\"CUDA is not available, disabling CUDA profiling\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                       Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg       CPU Mem  Self CPU Mem    # of Calls  \n",
      "---------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "            model_inference         0.63%     107.981ms       100.00%       17.135s       17.135s         480 b      -9.93 Gb             1  \n",
      "                aten::slice         0.01%       1.006ms         0.01%       1.028ms      20.565us           0 b           0 b            50  \n",
      "           aten::as_strided         0.13%      22.047ms         0.13%      22.047ms       0.157us           0 b           0 b        140360  \n",
      "            aten::unsqueeze         0.00%      47.622us         0.00%      55.580us       2.779us           0 b           0 b            20  \n",
      "                   aten::to         0.01%       1.025ms         0.26%      45.146ms      18.579us       1.58 Gb           0 b          2430  \n",
      "             aten::_to_copy         0.02%       2.869ms         0.26%      44.121ms      20.147us       1.58 Gb     121.64 Mb          2190  \n",
      "        aten::empty_strided         0.01%       1.836ms         0.01%       1.836ms       0.839us       1.47 Gb       1.47 Gb          2190  \n",
      "                aten::copy_         0.59%     101.121ms         0.59%     101.121ms      34.512us           0 b           0 b          2930  \n",
      "                 aten::rsub         0.00%     160.414us         0.00%     244.665us      24.466us      20.00 Kb           8 b            10  \n",
      "                  aten::sub         0.15%      25.314ms         0.15%      25.348ms      51.731us       1.41 Gb       1.41 Gb           490  \n",
      "---------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 17.135s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "\n",
    "with profile(\n",
    "    activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],\n",
    "    record_shapes=True,\n",
    "    with_stack=True,\n",
    "    profile_memory=True\n",
    ") as prof:\n",
    "    with record_function(\"model_inference\"):\n",
    "        with torch.no_grad():\n",
    "            for _ in range(10):   # run multiple iterations to get stable timings\n",
    "                outputs = model_opt(**inputs)\n",
    "\n",
    "print(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "DeMESj2rqu0T",
   "metadata": {
    "id": "DeMESj2rqu0T"
   },
   "outputs": [],
   "source": [
    "# {\n",
    "#   \"input_ids\": torch.Tensor(...).to(device),        # int64\n",
    "#   \"attention_mask\": torch.Tensor(...).to(device),   # float16 if chosen\n",
    "#   \"token_type_ids\": torch.Tensor(...).to(device)    # int64\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "R3R-GdicKPvj",
   "metadata": {
    "id": "R3R-GdicKPvj"
   },
   "outputs": [],
   "source": [
    "# from torch.profiler import tensorboard_trace_handler\n",
    "\n",
    "# with profile(\n",
    "#     activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],\n",
    "#     on_trace_ready=tensorboard_trace_handler(\"./log\"),\n",
    "#     record_shapes=True,\n",
    "#     profile_memory=True,\n",
    "#     with_stack=True\n",
    "# ) as prof:\n",
    "#     with record_function(\"model_inference\"):\n",
    "#         for _ in range(10):\n",
    "#             outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3237f461-50a7-4468-bb23-2c2ac007d17e",
   "metadata": {
    "id": "3237f461-50a7-4468-bb23-2c2ac007d17e"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d298a7c-51c4-4826-a941-b02e61f14dda",
   "metadata": {
    "id": "2d298a7c-51c4-4826-a941-b02e61f14dda"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
