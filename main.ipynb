{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b89161c1-4bb7-4f6a-be2e-d8e987e487fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install onnxruntime # for running on Arm-based CPU and/or macOS\n",
    "# !pip3 install onnxruntime-gpu # for running on GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b549e7e-7d22-4c98-ac22-586da5f22723",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bhuvanesh/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eed213ff-8678-4ea2-aa14-2e7d73479be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Ernie\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"nghuyong/ernie-1.0-base-zh\")\n",
    "model = AutoModel.from_pretrained(\"nghuyong/ernie-1.0-base-zh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d695c099-f10d-41aa-afd6-cabfbb5bdc00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Just like BERT, there are usually 12 or 24 stacked Transformer layers (depending on the model size).\n",
    "\n",
    "# 2. Each layer is an instance of a block like the following:\n",
    "# class TransformerBlock(nn.Module):\n",
    "#     def __init__(...):\n",
    "#         self.attention = MultiheadAttention(...)\n",
    "#         self.norm1 = LayerNorm(...)\n",
    "#         self.mlp = FeedForward(...)\n",
    "#         self.norm2 = LayerNorm(...)\n",
    "\n",
    "# 3. To optimize ERNIE end-to-end:\n",
    "# - Modify MultiheadAttention \n",
    "# - Ensure the TransformerBlock wraps around your optimized version.\n",
    "# - Your changes automatically propagate to all N layers. Is this true?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "62fe521a-ec94-44f2-ac21-501a74b14303",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access a single transformer block\n",
    "block = model.encoder.layer[0] # For BERT-style ERNIE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f5667d0d-7905-4326-b54c-df69a58f2da8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ErnieLayer(\n",
       "  (attention): ErnieAttention(\n",
       "    (self): ErnieSelfAttention(\n",
       "      (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (output): ErnieSelfOutput(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (intermediate): ErnieIntermediate(\n",
       "    (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "    (intermediate_act_fn): ReLU()\n",
       "  )\n",
       "  (output): ErnieOutput(\n",
       "    (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb280dd-9682-4910-b6ea-b050471f927a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fused QKV (Whatâ€™s happening internally?):\n",
    "# - The single Linear layer has shape [input_dim, 3 * embed_dim].\n",
    "# - During the forward pass, torch.chunk splits the output into 3 tensors.\n",
    "# - This results in 1 kernel call, as opposed to 3 separate linear layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c1a35703-d8e6-4147-a937-9c3fdb5e2afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backwards-compatible conversion\n",
    "query = block.attention.self.query\n",
    "key = block.attention.self.key\n",
    "value = block.attention.self.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4a078482-b489-4f9f-b320-9acb571f20de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Create your fused module\n",
    "fused_qkv = FusedQKV(input_dim=768, embed, num_heads=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0775a84c-217c-437e-9948-5010860d7e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Concatenate weights and biases\n",
    "# torch.cat() concatenates the given sequence of tensors in tensors in the given dimension\n",
    "# Weight: [3*embed_dim, input_dim] since Linear weight is (out, in)\n",
    "qkv_weight = torch.cat([\n",
    "    query.weight.data,\n",
    "    key.weight.data,\n",
    "    value.weight.data\n",
    "], dim=0)  # Shape: [3*768, 768]\n",
    "\n",
    "# Bias: [3*embed_dim]\n",
    "qkv_bias = torch.cat([\n",
    "    query.bias.data,\n",
    "    key.bias.data,\n",
    "    value.bias.data\n",
    "], dim=0)  # Shape: [3*768]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68bcb56d-3231-4846-88de-fc35a4e9de00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Assign to fused projection layer - This overrites the randonly initalized weights\\\n",
    "# in qkv_proj with the concatenated weights of Q, K, and V from the original ERNIE Layer.\n",
    "fused_qkv.qkv_proj.weight.data.copy_(qkv_weight)\n",
    "fused_qkv.qkv_proj.bias.data.copy_(qkv_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcfc0c8c-0880-4b54-b272-3951c543f4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stack them into one linear operation\n",
    "# QKV = W_qkv @ x + b_qkv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3af4ad0-c155-47cd-bca6-c1dd8c804479",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "37880589-7a04-4ab1-99e2-f7314968a8a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2ae2eb-4428-470b-b325-0955cd4384f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you're deploying this model efficiently:\n",
    "# - TorchScript will often fuse the operations automatically.\n",
    "# - Libraries like FlashAttention, xFormers, or FusedLinear (from Nvidia's APEX or Triton)\n",
    "# - offer fused QKV kernels with GPU-level optimizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab2d1bd-f479-4fe0-882c-ac3a7b5cfd99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c757500f-9a95-41f5-83e7-189f178ee2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore / overwrite the following components.\n",
    "# block.attention.self.dropout\n",
    "# block.attention.output.dense\n",
    "# block.attention.output.LayerNorm\n",
    "# block.attention.output.dropout\n",
    "# block.intermediate.dense\n",
    "# block.intermediate.intermediate_act_fn\n",
    "# block.output.dense\n",
    "# block.output.LayerNorm\n",
    "# block.output.dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92808c79-efee-49e6-8279-78c1904900c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ea9e155c-24f7-425b-9c63-a90d8deeaa2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# block_1 = model.encoder.layer[11] \n",
    "# block_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1919791-65a8-4005-b9ab-56416fb4b412",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f62380-3bfc-4985-a4dd-ef1433577f8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f50aaed-fb69-470f-ab59-b0d57e1f17cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec45a737-7757-408d-a868-738d659c7ff6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b61c031-ced8-4e69-b9cd-121e96a3fc8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "340be4c2-ec18-419f-b899-5bdd3088b4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Export to ONNX\n",
    "# dummy_input = torch.randint(0, 100, (1, 128))  # (batch, seq_len)\n",
    "# torch.onnx.export(\n",
    "#     model, (dummy_input,),\n",
    "#     \"ernie.onnx\",\n",
    "#     input_names=[\"input_ids\"],\n",
    "#     output_names=[\"output\"],\n",
    "#     dynamic_axes={\"input_ids\": {0: \"batch_size\", 1: \"seq_len\"}},\n",
    "#     opset_version=14\n",
    "# )\n",
    "\n",
    "# UnsupportedOperatorError: Exporting the operator 'aten::scaled_dot_product_attention'\n",
    "# to ONNX opset version 13 is not supported. Support for this operator was added in \n",
    "# version 14, try exporting with this version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f14b2b24-849c-4ce8-9f28-37f9fe197886",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify in ONNX Runtime\n",
    "# import onnxruntime as ort\n",
    "# ort.InferenceSession(\"ernie.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35b7b8d-9646-46c7-b771-7cce8a6533ef",
   "metadata": {},
   "source": [
    "https://docs.pytorch.org/docs/stable/onnx.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f76d5468-70dc-454a-9793-44136052d9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# An example for exporting a model from PyTorch to ONNX (Open Neural Network eXchange) \n",
    "# ONNX is an open standard format for representing ML models. The torch.onnx module \n",
    "# captures the computation graph from a native PyTorch torch.nn.Module model and convert\n",
    "# it into an ONNX graph.\n",
    "\n",
    "# class MyModel(torch.nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(MyModel, self).__init__()\n",
    "#         self.conv1 = torch.nn.Conv2d(1, 128, 5)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return torch.relu(self.conv1(x))\n",
    "\n",
    "# input_tensor = torch.rand((1, 1, 128, 128), dtype=torch.float32)\n",
    "\n",
    "# model = MyModel()\n",
    "# # model\n",
    "# # input_tensor.dtype\n",
    "\n",
    "# torch.onnx.export(\n",
    "#     model,                 # model to export\n",
    "#     (input_tensor,),       # inputs of the model\n",
    "#     \"my_model.onnx\",       # filename of the ONNX model\n",
    "#     input_names=[\"input\"], # Rename inputs for the ONNX model\n",
    "#     dynamo=True            # True or False o select the exporter to use\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3237f461-50a7-4468-bb23-2c2ac007d17e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d298a7c-51c4-4826-a941-b02e61f14dda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2cf52a-a6b3-4ee9-af96-24610a15a844",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2e0a54-7b63-49e1-9b1d-8a5f9a409f74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a930d8-80c8-4e7f-8c0e-c34646ff60d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95bf6a60-ace7-41cb-b03a-ddf97327ba25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c4ad64-0f85-4eef-935c-7579ccedde1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03bb0e12-ee46-49cd-9d17-6fc7d77ac1f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec58135-5d54-467c-8ebf-16817a05a3c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22eecf0-46a8-404e-99d4-dc60556cd9d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3182f732-e025-4f1c-9437-7b78a1ce6856",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f563d16-86f9-453f-8bc3-a14d209ea737",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d9a216-3e41-4139-947b-4aad19b2af70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baddbad8-03d5-482f-8d37-c4d5a45e9c11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7edc1dcd-a167-4dfd-9400-e1494f33b344",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f073b1-0224-4341-a6ee-919c584846bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1e8fc0-6d3c-411a-983f-d180feef853f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dface807-17cc-46b1-8428-8d29d2a570a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc9a643-e3b3-4a76-8b68-188907f447eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e775d7-1f50-4f60-892a-16df6da83f91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8134e99b-b94c-4879-99e4-f5fd9ba3ee9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f18199c-6ec0-4c6c-9fcc-7dab1458ae6e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
