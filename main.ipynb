{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdk5PXP3fnJz",
   "metadata": {
    "id": "fdk5PXP3fnJz"
   },
   "outputs": [],
   "source": [
    "# !pip install onnx onnxruntime onnxscript\n",
    "# !pip3 install nbstripout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b549e7e-7d22-4c98-ac22-586da5f22723",
   "metadata": {
    "id": "7b549e7e-7d22-4c98-ac22-586da5f22723"
   },
   "outputs": [],
   "source": [
    "# --- Imports ---\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "# Optional ONNX/TensorRT imports\n",
    "# import onnx\n",
    "# import onnxscript\n",
    "# import onnxruntime as ort\n",
    "\n",
    "# --- Add project root to path (adjust as needed) ---\n",
    "PROJECT_ROOT = \"LLM/ernie-tensorrt-inference\"\n",
    "if PROJECT_ROOT not in sys.path:\n",
    "    sys.path.append(PROJECT_ROOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2DYJqsW8y_qK",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 31,
     "status": "ok",
     "timestamp": 1755786783969,
     "user": {
      "displayName": "Athiyen Arivalagan",
      "userId": "03296907817711712766"
     },
     "user_tz": -480
    },
    "id": "2DYJqsW8y_qK",
    "outputId": "628be838-87f1-4124-f9f9-281ca6902f63"
   },
   "outputs": [],
   "source": [
    "# --- Device setup ---\n",
    "def get_device():\n",
    "    return torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "device = get_device()\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "XfQTe8PJ7kgF",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21,
     "status": "ok",
     "timestamp": 1755786783995,
     "user": {
      "displayName": "Athiyen Arivalagan",
      "userId": "03296907817711712766"
     },
     "user_tz": -480
    },
    "id": "XfQTe8PJ7kgF",
    "outputId": "c16dd362-e676-49d7-e8fd-4456e5277beb"
   },
   "outputs": [],
   "source": [
    "# --- Precision selection for optimized model ---\n",
    "PRECISION_OPTIONS = [\"float32\", \"float16\", \"bfloat16\"]\n",
    "precision = \"float16\" # change as needed\n",
    "assert precision in PRECISION_OPTIONS, f\"Precision must be one of {PRECISION_OPTIONS}\"\n",
    "print(f\"Optimized Model Precision: {precision}\")\n",
    "\n",
    "# --- Batch size selection ---\n",
    "BATCH_SIZE_OPTIONS = [16, 32, 64, 128, 512]\n",
    "batch_size = 16 # change as needed\n",
    "assert batch_size in BATCH_SIZE_OPTIONS, f\"Batch size must be one of {BATCH_SIZE_OPTIONS}\"\n",
    "print(f\"Batch size: {batch_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "XXxroDlmUKtm",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3930,
     "status": "ok",
     "timestamp": 1755786787926,
     "user": {
      "displayName": "Athiyen Arivalagan",
      "userId": "03296907817711712766"
     },
     "user_tz": -480
    },
    "id": "XXxroDlmUKtm",
    "outputId": "d5ad3d63-39db-4f4e-a1ed-3523d616c484"
   },
   "outputs": [],
   "source": [
    "# --- Load validation dataset ---\n",
    "def load_validation_dataset(dataset_name=\"C-MTEB/TNews-classification\", split=\"validation\"):\n",
    "    ds = load_dataset(dataset_name)\n",
    "    return ds[split]\n",
    "\n",
    "val_data = load_validation_dataset()\n",
    "print(f\"Validation dataset size: {len(val_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rFk6BHqTl42a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6005,
     "status": "ok",
     "timestamp": 1755786793934,
     "user": {
      "displayName": "Athiyen Arivalagan",
      "userId": "03296907817711712766"
     },
     "user_tz": -480
    },
    "id": "rFk6BHqTl42a",
    "outputId": "7f8d1bac-26bc-49d8-a226-50cc734acf8a"
   },
   "outputs": [],
   "source": [
    "# --- Model setup ---\n",
    "def load_models(model_name=\"nghuyong/ernie-3.0-base-zh\", num_labels=15):\n",
    "    # Original model (baseline)\n",
    "    model_orig = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n",
    "\n",
    "    # Optimized model (for future TensorRT / fused layers)\n",
    "    model_opt = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n",
    "\n",
    "    return model_orig, model_opt\n",
    "\n",
    "# --- Load models ---\n",
    "model_orig, model_opt = load_models(num_labels=15)\n",
    "\n",
    "# --- Print configuration info ---\n",
    "config = model_orig.config\n",
    "print(f\"Number of labels: {config.num_labels}\") # outputs: 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ikNblAU4YD8F",
   "metadata": {
    "id": "ikNblAU4YD8F"
   },
   "outputs": [],
   "source": [
    "from tokenizer import tokenize_function\n",
    "\n",
    "# --- Tokenize + prepare DataLoader ---\n",
    "def prepare_dataloader(dataset, batch_size=batch_size, max_length=64, shuffle=False):\n",
    "    tokenized = dataset.map(lambda x: tokenize_function(x, max_length=max_length), batched=True)\n",
    "    tokenized.set_format(\n",
    "        type=\"torch\",\n",
    "        columns=[\"input_ids\", \"attention_mask\", \"token_type_ids\", \"label\"]\n",
    "    )\n",
    "    return DataLoader(tokenized, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "# --- Create DataLoader ---\n",
    "val_loader = prepare_dataloader(val_data, batch_size=batch_size, max_length=64, shuffle=False)\n",
    "\n",
    "# Note: smaller max_length (e.g., 64 or 128 vs default 512) reduces memory and boosts inference speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9efd48d3-0ab6-462a-add7-3cfbba543946",
   "metadata": {
    "id": "9efd48d3-0ab6-462a-add7-3cfbba543946"
   },
   "outputs": [],
   "source": [
    "from models import ErnieEmbeddings, ErnieSelfAttention, ErnieSelfOutput, ErnieIntermediate, ErnieOutput, ErniePooler\n",
    "\n",
    "# --- Replace ERNIE model components for optimization ---\n",
    "def replace_ernie_layers(model, config):\n",
    "    # Replace embeddings\n",
    "    model.ernie.embeddings = ErnieEmbeddings(config)\n",
    "\n",
    "    # Replace each encoder layer's components\n",
    "    for layer in model.ernie.encoder.layer:\n",
    "        layer.attention.self = ErnieSelfAttention(config)\n",
    "        layer.attention.output = ErnieSelfOutput(config)\n",
    "        layer.intermediate = ErnieIntermediate(config)\n",
    "        layer.output = ErnieOutput(config)\n",
    "        layer.pooler = ErniePooler(config)\n",
    "\n",
    "    model.ernie.pooler = ErniePooler(config)\n",
    "\n",
    "    return model\n",
    "\n",
    "model_opt = replace_ernie_layers(model_opt, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "WuU5h0KW1qYM",
   "metadata": {
    "id": "WuU5h0KW1qYM"
   },
   "outputs": [],
   "source": [
    "# --- Helper function to prepare model ---\n",
    "def prepare_model(model, device):\n",
    "    model.eval()\n",
    "    return model.to(device)\n",
    "\n",
    "# --- Apply to original and optimized models ---\n",
    "model_orig = prepare_model(model_orig, device)\n",
    "model_opt = prepare_model(model_opt, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af0a164-8332-441b-b997-7f5ab3ddccaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the structure of the optimized model\n",
    "model_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mQXfsdWrOsaN",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22,
     "status": "ok",
     "timestamp": 1755786796489,
     "user": {
      "displayName": "Athiyen Arivalagan",
      "userId": "03296907817711712766"
     },
     "user_tz": -480
    },
    "id": "mQXfsdWrOsaN",
    "outputId": "669fa309-390e-44a8-9be0-1eb3b627c75d"
   },
   "outputs": [],
   "source": [
    "# --- Set model precision ---\n",
    "def set_model_precision(model, precision=\"float32\"):\n",
    "    if precision == \"float16\":\n",
    "        model = model.half()\n",
    "    elif precision == \"bfloat16\":\n",
    "        model = model.to(torch.bfloat16)\n",
    "    # float32 requires no action\n",
    "    return model\n",
    "\n",
    "model_opt = set_model_precision(model_opt, precision)\n",
    "\n",
    "# --- Print model info ---\n",
    "def print_model_info(model_orig, model_opt, precision):\n",
    "    print(f\"Original Model Device: {model_orig.device}\")\n",
    "    print(f\"Optimized Model Device: {model_opt.device}\")\n",
    "    print(f\"Optimized Model Precision: {model_opt.dtype} ({precision})\")\n",
    "\n",
    "# print_model_info(model_orig, model_opt, precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70FZHkoe7gv",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22619,
     "status": "ok",
     "timestamp": 1755786819110,
     "user": {
      "displayName": "Athiyen Arivalagan",
      "userId": "03296907817711712766"
     },
     "user_tz": -480
    },
    "id": "e70FZHkoe7gv",
    "outputId": "94f63709-3b1e-40ce-9407-b98a75ab1912"
   },
   "outputs": [],
   "source": [
    "# --- Loss function ---\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# --- Metric computation ---\n",
    "def evaluate_model(model, data_loader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            # Move inputs and labels to device\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            token_type_ids = batch[\"token_type_ids\"].to(device)\n",
    "            labels = batch[\"label\"].to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                token_type_ids=token_type_ids\n",
    "            )\n",
    "\n",
    "            # Use outputs[0] for models without logits attribute\n",
    "            logits = outputs[0] # shape: [batch_size, num_classes]\n",
    "\n",
    "            # Loss\n",
    "            loss = criterion(logits, labels)\n",
    "            total_loss += loss.item() * input_ids.size(0)\n",
    "\n",
    "            # Accuracy\n",
    "            preds = logits.argmax(dim=-1)\n",
    "            total_correct += (preds == labels).sum().item()\n",
    "            total_samples += input_ids.size(0)\n",
    "\n",
    "    avg_loss = total_loss / total_samples\n",
    "    accuracy = total_correct / total_samples\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "# --- Latency benchmarking ---\n",
    "def measure_latency(model, inputs, device, n_warmup=10, n_iter=100):\n",
    "    latencies = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Warm-up\n",
    "        for _ in range(n_warmup):\n",
    "            _ = model(**inputs)\n",
    "            torch.cuda.synchronize()\n",
    "\n",
    "        # Measure\n",
    "        for _ in range(n_iter):\n",
    "            torch.cuda.synchronize()\n",
    "            start = time.time()\n",
    "            _ = model(**inputs)\n",
    "            torch.cuda.synchronize()\n",
    "            end = time.time()\n",
    "            latencies.append((end - start) * 1000) # ms\n",
    "\n",
    "    mean_latency = np.mean(latencies)\n",
    "    return mean_latency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5621d8-8eb2-4306-a2dd-a1ca18cfbaae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Prepare a single batch for latency measurement ---\n",
    "sample_batch = next(iter(val_loader))\n",
    "# inputs = {k: v.to(device) for k, v in sample_batch.items() if k in [\"input_ids\", \"attention_mask\", \"token_type_ids\"]}\n",
    "\n",
    "# Convert only float32 tensors to FP16, keep int64 (like input_ids) untouched\n",
    "inputs = {\n",
    "    k: (v.to(device).half() if v.dtype == torch.float32 else v.to(device))\n",
    "    for k, v in sample_batch.items()\n",
    "    if k in [\"input_ids\", \"attention_mask\", \"token_type_ids\"]}\n",
    "\n",
    "# --- Evaluate Original Model ---\n",
    "avg_loss_orig, acc_orig = evaluate_model(model_orig, val_loader, device)\n",
    "latency_orig = measure_latency(model_orig, inputs, device)\n",
    "\n",
    "print(f\"[Original] Precision: {model_orig.dtype}, Loss: {avg_loss_orig:.4f}, Accuracy: {acc_orig:.4f}, Mean latency: {latency_orig:.2f} ms\")\n",
    "\n",
    "# --- Evaluate Optimized Model ---\n",
    "avg_loss_opt, acc_opt = evaluate_model(model_opt, val_loader, device)\n",
    "latency_opt = measure_latency(model_opt, inputs, device)\n",
    "\n",
    "print(f\"[Optimized] Precision: {model_opt.dtype}, Loss: {avg_loss_opt:.4f}, Accuracy: {acc_opt:.4f}, Mean latency: {latency_opt:.2f} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Fn8JY2apJ_rr",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3941,
     "status": "ok",
     "timestamp": 1755786823071,
     "user": {
      "displayName": "Athiyen Arivalagan",
      "userId": "03296907817711712766"
     },
     "user_tz": -480
    },
    "id": "Fn8JY2apJ_rr",
    "outputId": "f53089eb-a00e-41a1-a09e-5b719656c426"
   },
   "outputs": [],
   "source": [
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "\n",
    "with profile(\n",
    "    activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],\n",
    "    record_shapes=True,\n",
    "    with_stack=True,\n",
    "    profile_memory=True\n",
    ") as prof:\n",
    "    with record_function(\"model_inference\"):\n",
    "        with torch.no_grad():\n",
    "            for _ in range(10):   # run multiple iterations to get stable timings\n",
    "                outputs = model_opt(**inputs)\n",
    "\n",
    "print(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "DeMESj2rqu0T",
   "metadata": {
    "id": "DeMESj2rqu0T"
   },
   "outputs": [],
   "source": [
    "# {\n",
    "#   \"input_ids\": torch.Tensor(...).to(device),        # int64\n",
    "#   \"attention_mask\": torch.Tensor(...).to(device),   # float16 if chosen\n",
    "#   \"token_type_ids\": torch.Tensor(...).to(device)    # int64\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "R3R-GdicKPvj",
   "metadata": {
    "id": "R3R-GdicKPvj"
   },
   "outputs": [],
   "source": [
    "# from torch.profiler import tensorboard_trace_handler\n",
    "\n",
    "# with profile(\n",
    "#     activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],\n",
    "#     on_trace_ready=tensorboard_trace_handler(\"./log\"),\n",
    "#     record_shapes=True,\n",
    "#     profile_memory=True,\n",
    "#     with_stack=True\n",
    "# ) as prof:\n",
    "#     with record_function(\"model_inference\"):\n",
    "#         for _ in range(10):\n",
    "#             outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3237f461-50a7-4468-bb23-2c2ac007d17e",
   "metadata": {
    "id": "3237f461-50a7-4468-bb23-2c2ac007d17e"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
